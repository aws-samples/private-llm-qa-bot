import json
import logging
import time
import os
import re
import io
from botocore import config
from botocore.exceptions import ClientError,EventStreamError
from datetime import datetime, timedelta
import boto3
import uuid
import base64
from enum import Enum
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from pydantic import BaseModel,Field

import openai
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult
from typing import Any, Dict, List, Union,Mapping, Optional, TypeVar, Union
from langchain.schema import LLMResult
from enum import Enum
from boto3 import client as boto3_client
from utils.management import management_api,get_template
from utils.utils import add_reference, render_answer_with_ref
from generator.llm_wrapper import get_langchain_llm_model, invoke_model, format_to_message
from retriever.hybrid_retriever import CustomDocRetriever

lambda_client= boto3.client('lambda')
dynamodb_client = boto3.resource('dynamodb')
region = boto3.Session().region_name

DOC_INDEX_TABLE= 'chatbot_doc_index'
logger = logging.getLogger()
logger.setLevel(logging.INFO)
lambda_client = boto3_client('lambda')
chat_session_table = os.environ.get('chat_session_table')
QA_SEP = "=>"
A_Role="ç”¨æˆ·"
B_Role="AWSBot"
A_Role_en="user"
SYSTEM_ROLE_PROMPT = 'ä½ æ˜¯äº‘æœåŠ¡AWSçš„æ™ºèƒ½å®¢æœæœºå™¨äººAWSBot'
Fewshot_prefix_Q="é—®é¢˜"
Fewshot_prefix_A="å›ç­”"
RESET = '/rs'
openai_api_key = None
STOP=[f"\n{A_Role_en}", f"\n{A_Role}", f"\n{Fewshot_prefix_Q}", '</response>']
CHANNEL_RET_CNT = 10
SESSION_EXPIRES_DAYS = 1

BM25_QD_THRESHOLD_HARD_REFUSE = float(os.environ.get('bm25_qd_threshold_hard',15.0))
BM25_QD_THRESHOLD_SOFT_REFUSE = float(os.environ.get('bm25_qd_threshold_soft',20.0))
KNN_QQ_THRESHOLD_HARD_REFUSE = float(os.environ.get('knn_qq_threshold_hard',0.6))
KNN_QQ_THRESHOLD_SOFT_REFUSE = float(os.environ.get('knn_qq_threshold_soft',0.8))
KNN_QD_THRESHOLD_HARD_REFUSE = float(os.environ.get('knn_qd_threshold_hard',0.6))
KNN_QD_THRESHOLD_SOFT_REFUSE = float(os.environ.get('knn_qd_threshold_soft',0.8))
RERANK_THRESHOLD = float(os.environ.get('rerank_threshold_soft',-2))
WEBSEARCH_THRESHOLD = float(os.environ.get('websearch_threshold_soft',1))
CROSS_MODEL_ENDPOINT = os.environ.get('cross_model_endpoint',None)
KNN_QUICK_PEFETCH_THRESHOLD = float(os.environ.get('knn_quick_prefetch_threshold',0.95))

INTENTION_LIST = os.environ.get('intention_list', "")

TOP_K = int(os.environ.get('TOP_K',4))
NEIGHBORS = int(os.environ.get('neighbors',0))
KNOWLEDGE_BASE_ID = os.environ.get('knowledge_base_id',None)

BEDROCK_LLM_MODELID_LIST = {'claude-instant':'anthropic.claude-instant-v1',
                            'claude-v2':'anthropic.claude-v2',
                            'claude-v3-sonnet': 'anthropic.claude-3-sonnet-20240229-v1:0',
                            'claude-v3-haiku' : 'anthropic.claude-3-haiku-20240307-v1:0',
                            'llama3-70b': 'meta.llama3-70b-instruct-v1:0',
                            'llama3-8b': 'meta.llama3-8b-instruct-v1:0'}

###è®°å½•è·Ÿè¸ªæ—¥å¿—ï¼Œç”¨äºå‰ç«¯è¾“å‡º
class TraceLogger(BaseModel):
    logs:List[str] =  Field([])
    ref_docs:List[str] = Field([])
    wsclient:Any = Field()
    connectionId:str = Field()
    msgid:str=Field()
    stream:bool = Field()
    use_trace:bool=Field()
    hide_ref:bool=Field()
    class Config:
        extra = 'forbid'
    
    def postMessage(self,text:str) -> None:
        data = json.dumps({ 'msgid':self.msgid, 'role': "AI", 'text': {'content':f'{text}\n\n'},'connectionId':self.connectionId })
        try:
            self.wsclient.post_to_connection(Data = data.encode('utf-8'),  ConnectionId=self.connectionId)
        except Exception as e:
            logger.warning(str(e))
            
    def add_ref(self,text:str) -> None:
        self.ref_docs.append(text)

    def trace(self,text:str) -> None:
        if not self.use_trace:
            return
        self.logs.append(text)
        if self.stream:
            self.postMessage(text)
        
    ##ref docæ’åœ¨llmè¾“å‡ºanswerä¹‹åï¼Œä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„streamå‚æ•°æ§åˆ¶æ˜¯å¦éœ€è¦æ¨åˆ°ws
    def dump_refs(self,stream) -> List[str]:
        if stream and not self.use_trace and not self.hide_ref: ##å½“ä¸ä½¿ç”¨traceï¼Œä¸éšè—refæ—¶æ‰æ¨é€
            for text in self.ref_docs:
                self.postMessage(text)
        return '\n\n'.join(self.ref_docs)
    
    def dump_logs_to_string(self) -> List[str]:
        return '\n\n'.join(self.logs)
    
TRACE_LOGGER = None

class CustomStreamingOutCallbackHandler(BaseCallbackHandler):
    """Callback handler for streaming. Only works with LLMs that support streaming."""
    def __init__(self,wsclient:str,msgid:str,connectionId:str ,model_name:str,hide_ref:bool,use_stream:bool, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self.wsclient = wsclient
        self.connectionId = connectionId
        self.msgid = msgid
        self.model_name= model_name
        self.recall_knowledge = []
        self.hide_ref = hide_ref
        self.use_stream = use_stream

    def add_recall_knowledge(self,recall_knowledge):

        self.recall_knowledge = recall_knowledge

    def postMessage(self,data):
        try:
            self.wsclient.post_to_connection(Data = data.encode('utf-8'),  ConnectionId=self.connectionId)
        except Exception as e:
            pass
            # print (f'post {json.dumps(data)} to_wsconnection error:{str(e)}')

    def message_format(self,messages):
        """Format messages as ChatGPT who only accepts roles of ['system', 'assistant', 'user']"""
        return [
            {'role': 'assistant', 'content': msg['content']}
            if msg['role'] == 'AI'
            else {'role': 'user', 'content': msg['content']}
            for msg in messages
        ]
    
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when LLM starts running."""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Run on new LLM token. Only available when streaming is enabled."""
        data = json.dumps({ 'msgid':self.msgid, 'role': "AI", 'text': {'content':token},'connectionId':self.connectionId})
        self.postMessage(data)

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when LLM ends running."""
        if (not self.hide_ref) and self.use_stream:
            text = format_reference(self.recall_knowledge)
            data = json.dumps({ 'msgid':self.msgid, 'role': "AI", 'text': {'content':f'{text}'},'connectionId':self.connectionId })
            self.postMessage(data)

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        data = json.dumps({ 'msgid':self.msgid, 'role': "AI", 'text': {'content':str(error[0])+'[DONE]'},'connectionId':self.connectionId})
        self.postMessage(data)

class ReplyStratgy(Enum):
    LLM_ONLY = 1
    WITH_LLM = 2
    HINT_LLM_REFUSE = 3
    RETURN_OPTIONS = 4
    SAY_DONT_KNOW = 5
    AGENT = 6
    OTHER = 7


class APIException(Exception):
    def __init__(self, message, code: str = None):
        if code:
            super().__init__("[{}] {}".format(code, message))
        else:
            super().__init__(message)


def handle_error(func):
    """Decorator for exception handling"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except APIException as e:
            logger.exception(e)
            raise e
        except Exception as e:
            logger.exception(e)
            raise RuntimeError(
                "Unknown exception, please check Lambda log for more details"
            )

    return wrapper

def detect_intention(query, example_index='chatbot-example-index',fewshot_cnt=5):
    msg = {"fewshot_cnt":fewshot_cnt, "query": query,"example_index":example_index}
    invoke_response = lambda_client.invoke(FunctionName="Detect_Intention",
                                           InvocationType='RequestResponse',
                                           Payload=json.dumps(msg))
    response_body = invoke_response['Payload']

    response_str = response_body.read().decode("unicode_escape")
    response_str = response_str.strip('"')

    return json.loads(response_str)

def rewrite_query(query, session_history, round_cnt=2):
    logger.info(f"session_history {str(session_history)}")
    if len(session_history) == 0:
        return query

    history = []
    for item in session_history[-1 * round_cnt:]:
        history.append(item[0])
        history.append(item[1])

    msg = {
      "params": {
        "history": history,
        "query": query
      }
    }
    response = lambda_client.invoke(FunctionName="Query_Rewrite",
                                           InvocationType='RequestResponse',
                                           Payload=json.dumps(msg))
    response_body = response['Payload']
    response_str = response_body.read().decode("unicode_escape")

    return response_str.strip('"')

def chat_agent(query, detection):

    msg = {
      "params": {
        "query": query,
        "detection": detection 
      }
    }
    response = lambda_client.invoke(FunctionName="Chat_Agent",
                                           InvocationType='RequestResponse',
                                           Payload=json.dumps(msg))
    payload_json = json.loads(response.get('Payload').read())
    body = payload_json['body']
    answer = body['answer']
    ref_doc = body['ref_doc']

    return answer,ref_doc



def delete_session(session_id,user_id):
    # dynamodb = boto3.resource('dynamodb')
    table = dynamodb_client.Table(chat_session_table)
    try:
        table.delete_item(
        Key={
            'session-id': session_id,
            'user_id':user_id
        })
    except Exception as e:
        logger.info(f"delete session failed {str(e)}")

        
def get_session(session_id,user_id):

    table_name = chat_session_table
    # dynamodb = boto3.resource('dynamodb')

    # table name
    table = dynamodb_client.Table(table_name)
    operation_result = ""
    try:
        response = table.get_item(Key={'session-id': session_id,'user_id':user_id})
        if "Item" in response.keys():
        # print("****** " + response["Item"]["content"])
            operation_result = json.loads(response["Item"]["content"])
        else:
            # print("****** No result")
            operation_result = ""
        return operation_result
    except Exception as e:
        logger.info(f"get session failed {str(e)}")
        return ""

# param:    session_id
#           question
#           answer
# return:   success
#           failed
def update_session(session_id,user_id,msgid, question, answer, intention):

    table_name = chat_session_table
    # dynamodb = boto3.resource('dynamodb')

    # table name
    table = dynamodb_client.Table(table_name)
    operation_result = ""

    response = table.get_item(Key={'session-id': session_id,'user_id':user_id})

    if "Item" in response.keys():
        # print("****** " + response["Item"]["content"])
        chat_history = json.loads(response["Item"]["content"])
    else:
        # print("****** No result")
        chat_history = []

    timestamp_str = str(datetime.now())
    expire_at = int(time.time())+3600*24*SESSION_EXPIRES_DAYS #session expires in 1 days 
    
    chat_history = [item for item in chat_history if len(item) >=6 and item[5] > int(time.time())]
    
    chat_history.append([question, answer, intention,msgid,timestamp_str,expire_at])
    content = json.dumps(chat_history,ensure_ascii=False)

    # inserting values into table
    response = table.put_item(
        Item={
            'session-id': session_id,
            'user_id':user_id,
            'content': content,
            'last_updatetime':timestamp_str,
            'expire_at':expire_at
        }
    )

    if "ResponseMetadata" in response.keys():
        if response["ResponseMetadata"]["HTTPStatusCode"] == 200:
            operation_result = "success"
        else:
            operation_result = "failed"
    else:
        operation_result = "failed"

    return operation_result

def enforce_stop_tokens(text: str, stop: List[str]) -> str:
    """Cut off the text as soon as any stop words occur."""
    if stop is None:
        return text
    
    return re.split("|".join(stop), text)[0]

def format_knowledges(recalls):
    knowledges = []
    multi_choice_field = []
    meta_dict = {}
    for idx, item in enumerate(recalls):
        if len(item.get('doc_meta','')) > 0:
            meta_obj = json.loads(item['doc_meta'])
            for k, v in meta_obj.items():
                if k in meta_dict.keys() and meta_dict[k] != v:
                    multi_choice_field.append(k)
                else:
                    meta_dict[k] = v
            item_obj = { "meta" : meta_obj, 'text': item['doc']}
            content = json.dumps(item_obj, ensure_ascii=False)
            item_str = f"""<item index="{idx+1}">{content}</item>"""
        else:
            item_obj = {'text': item['doc']}
            content = json.dumps(item_obj, ensure_ascii=False)
            item_str = f"""<item index="{idx+1}">{content}</item>"""
        knowledges.append(item_str)

    context_str = "\n".join(knowledges)
    return context_str, set(multi_choice_field)


def get_question_history(inputs) -> str:
    res = []
    for human, _ in inputs:
        res.append(f"{human}\n")
    return "\n".join(res)

def get_qa_history(inputs) -> str:
    res = []
    for human, ai in inputs:
        res.append(f"{human}:{ai}\n")
    return "\n".join(res)

def get_chat_history(inputs) -> str:
    res = []
    for human, ai in inputs:
        res.append(f"{A_Role}:{human}\n{B_Role}:{ai}")
    return "\n".join(res)

def history_to_messages(inputs) -> str:
    res = []
    for human, ai in inputs:
        res.append({ "role" : "user", "content" : human})
        res.append({ "role" : "assistant", "content" : ai})
    return res

def create_qa_prompt_templete(prompt_template):
    if prompt_template == '':
        prompt_template_zh = \
"""Human: {system_role_prompt}{role_bot}Here is a query:
{chat_history}
<query>
{question}
</query>

Below may contains some relevant information to the query:

<information>
{context}
</information>

Once again, the user's query is:

<query>
{question}
</query>

Please follow below requirements:{ask_user_prompt}
- Respond in the original language of the question.
- Please try you best to leverage the image and hyperlink provided in <information>, you need to keep them in Markdown format.
- Do not directly reference the content of <information> in your answer.
- Skip the preamble, go straight into the answer. The answers will strictly be based on relevant knowledge in <information>.
- if the information is empty or not relevant to user's query, then reponse don't know.

Assistant:"""
    else:
        prompt_template_zh = prompt_template + '{ask_user_prompt}'
    PROMPT = PromptTemplate(
        template=prompt_template_zh,
        partial_variables={'system_role_prompt':SYSTEM_ROLE_PROMPT},
        input_variables=["context",'question','chat_history', 'role_bot', 'ask_user_prompt']
    )
    return PROMPT

def create_chat_prompt_templete(prompt_template='', llm_model_name='claude'):
    PROMPT = None
    if llm_model_name.startswith('claude'):
        prompt_template_zh = """Human: {system_role_prompt}{role_bot}Here is the conversation history (between the user and you) prior to the question. It could be empty if there is no history:
<history> {chat_history} </history>
Here is the userâ€™s question: <question> {question} </question>
How do you respond to the userâ€™s question?
Think about your answer first before you respond.
Assistant:"""
        PROMPT = PromptTemplate(
            template=prompt_template_zh, 
            partial_variables={'system_role_prompt':SYSTEM_ROLE_PROMPT},
            input_variables=['question', 'chat_history','role_bot']
        )
    elif llm_model_name.startswith('llama3'):
        prompt_template_zh = """
            <|begin_of_text|><|start_header_id|>system<|end_header_id|>
            {system_role_prompt}{role_bot}<|eot_id|><|start_header_id|>user<|end_header_id|>
            Here is the conversation history (between the user and you) prior to the question. It could be empty if there is no history:
            <history> {chat_history} </history>
            Here is the userâ€™s question: <question> {question} </question>
            <|eot_id|><|start_header_id|>assistant<|end_header_id|>
            """
        PROMPT = PromptTemplate(
            template=prompt_template_zh,
            partial_variables={'system_role_prompt': SYSTEM_ROLE_PROMPT},
            input_variables=['question', 'chat_history', 'role_bot']
        )
    else:
        if prompt_template == '':
            prompt_template_zh = """Human:{system_role_prompt}{role_bot}\n{chat_history}\n\n{question}"""
        else:
            prompt_template_zh = prompt_template.replace('{context}','') ##remove{context}
        PROMPT = PromptTemplate(
            template=prompt_template_zh, 
            partial_variables={'system_role_prompt':SYSTEM_ROLE_PROMPT},
            input_variables=['question','chat_history','role_bot']
        )
    return PROMPT

def format_reference(recall_knowledge):
    if not recall_knowledge:
        return ''
    text = '\n```json\n#Reference\n'
    for sn,item in enumerate(recall_knowledge):
        displaydata = { "doc": item['doc'],"score": item['score']}
        doc_category  = item['doc_classify']
        doc_title =  item['doc_title']
        text += f'Doc[{sn+1}]:["{doc_title}"]-["{doc_category}"]\n{json.dumps(displaydata,ensure_ascii=False)}\n'
    text += '\n```'
    return text

def get_reply_stratgy(recall_knowledge,refuse_strategy:str):
    if not recall_knowledge:##å¦‚æœå¸Œæœ›LLMåˆ©ç”¨è‡ªæœ‰çŸ¥è¯†å›ç­”ï¼Œåˆ™æ”¹æˆLLM_ONLY,å¦åˆ™SAY_DONT_KNOW
        if refuse_strategy == 'SAY_DONT_KNOW':
            stratgy = ReplyStratgy.SAY_DONT_KNOW
        elif refuse_strategy == 'WITH_LLM':
            stratgy = ReplyStratgy.WITH_LLM
        else:
            stratgy = ReplyStratgy.LLM_ONLY 
        return stratgy

    ## å¦‚æœä½¿ç”¨äº†rerankæ¨¡å‹
    if CROSS_MODEL_ENDPOINT:
        rank_score = [item['rank_score'] for item in recall_knowledge]
        if max(rank_score) < RERANK_THRESHOLD:  ##å¦‚æœæ‰€æœ‰çš„çŸ¥è¯†éƒ½ä¸è¶…è¿‡rank scoreé˜ˆå€¼
            ##å¦‚æœå¸Œæœ›LLMåˆ©ç”¨è‡ªæœ‰çŸ¥è¯†å›ç­”ï¼Œåˆ™æ”¹æˆLLM_ONLYï¼Œå¦åˆ™SAY_DONT_KNOW
            if refuse_strategy == 'SAY_DONT_KNOW':
                stratgy = ReplyStratgy.SAY_DONT_KNOW
            elif refuse_strategy == 'WITH_LLM':
                stratgy = ReplyStratgy.WITH_LLM
            else:
                stratgy = ReplyStratgy.LLM_ONLY 
            return stratgy
        else:
            return ReplyStratgy.WITH_LLM
    else:
        ##ä½¿ç”¨rerankä¹‹åï¼Œä¸éœ€è¦è¿™äº›ç­–ç•¥
        stratgy = ReplyStratgy.RETURN_OPTIONS
        for item in recall_knowledge:
            if item['score'] > 1.0:
                if item['score'] > BM25_QD_THRESHOLD_SOFT_REFUSE:
                    stratgy = ReplyStratgy.WITH_LLM
                elif item['score'] > BM25_QD_THRESHOLD_HARD_REFUSE:
                    stratgy = ReplyStratgy(min(ReplyStratgy.HINT_LLM_REFUSE.value, stratgy.value))
                else:
                    stratgy = ReplyStratgy(min(ReplyStratgy.RETURN_OPTIONS.value, stratgy.value))

            elif item['score'] <= 1.0:
                if item['score'] > KNN_QD_THRESHOLD_SOFT_REFUSE:
                    stratgy = ReplyStratgy.WITH_LLM
                elif item['score'] > KNN_QD_THRESHOLD_HARD_REFUSE:
                    stratgy = ReplyStratgy(min(ReplyStratgy.HINT_LLM_REFUSE.value, stratgy.value))
                else:
                    stratgy = ReplyStratgy(min(ReplyStratgy.RETURN_OPTIONS.value, stratgy.value))
        return stratgy
            
            
def main_entry_new(user_id:str,wsconnection_id:str,session_id:str, query_input:str, embedding_model_endpoint:str, llm_model_endpoint:str, llm_model_name:str, aos_endpoint:str, aos_index:str, aos_knn_field:str, aos_result_num:int, kendra_index_id:str, 
                   kendra_result_num:int,use_qa:bool,wsclient=None,msgid:str='',max_tokens:int = 2048,temperature:float = 0.1,template:str = '',images_base64:List[str] = None,multi_rounds:bool = False, hide_ref:bool = False,use_stream:bool=False,example_index:str='chatbot-example-index',use_search:bool=True,refuse_strategy:str = 'LLM_ONLY',refuse_answer:str = ''):
    """
    Entry point for the Lambda function.

    Parameters:
        session_id (str): The ID of the session.
        query_input (str): The query input.
        embedding_model_endpoint (str): The endpoint of the embedding model.
        llm_model_endpoint (str): The endpoint of the language model.
        aos_endpoint (str): The endpoint of the AOS engine.
        aos_index (str): The index of the AOS engine.
        aos_knn_field (str): The knn field of the AOS engine.
        aos_result_num (int): The number of results of the AOS engine.
        kendra_index_id (str): The ID of the Kendra index.
        kendra_result_num (int): The number of results of the Kendra Service.

    return: answer(str)
    """
    # STOP=[f"\n{A_Role}", f"\n{B_Role}"]
    global STOP,TRACE_LOGGER
    #å¦‚æœæ˜¯resetå‘½ä»¤ï¼Œåˆ™æ¸…ç©ºå†å²èŠå¤©
    if query_input == RESET:
        delete_session(session_id,user_id)
        answer = 'å†å²å¯¹è¯å·²æ¸…ç©º'
        json_obj = {
            "query": query_input,
            "opensearch_doc":  [],
            "opensearch_knn_doc":  [],
            "kendra_doc": [],
            "knowledges" : [],
            "detect_query_type": '',
            "LLM_input": '',
            "use_search":use_search
        }
        json_obj['user_id'] = user_id
        json_obj['session_id'] = session_id
        json_obj['chatbot_answer'] = answer
        json_obj['conversations'] = []
        json_obj['timestamp'] = int(time.time())
        json_obj['log_type'] = "all"
        json_obj_str = json.dumps(json_obj, ensure_ascii=False)
        logger.info(json_obj_str)
        use_stream = False
        return answer,'',use_stream,'',[],[],[]
    
    logger.info("llm_model_name : {} ,use_stream :{}".format(llm_model_name,use_stream))
    llm = None


    stream_callback = CustomStreamingOutCallbackHandler(wsclient,msgid, wsconnection_id,llm_model_name,hide_ref,use_stream)

    params = {
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p":0.95
    }

    if llm_model_name.startswith('claude') or llm_model_name.startswith('llama'):
        model_id = BEDROCK_LLM_MODELID_LIST.get(llm_model_name, BEDROCK_LLM_MODELID_LIST['claude-v3-sonnet'])
    else:
        model_id = llm_model_endpoint
    logger.info("current model_id : {}".format(model_id))
    llm = get_langchain_llm_model(model_id, params, region, llm_stream=use_stream, llm_callbacks=[])
    
    # 1. get_session
    start1 = time.time()
    session_history = get_session(session_id=session_id,user_id=user_id)

    chat_coversions = [ (item[0],item[1]) for item in session_history]

    elpase_time = time.time() - start1
    logger.info(f'running time of get_session : {elpase_time}s seconds')
    answer = None
    query_type = None
    # free_chat_coversions = []
    verbose = False
    logger.info(f'use QA: {use_qa}')
    final_prompt = ''
    origin_query = query_input
    intention = ''

    TRACE_LOGGER.trace(f'**Starting trace mode...**')
    if multi_rounds:
        before_rewrite = time.time()
        query_input = rewrite_query(origin_query, session_history, round_cnt=3)
        elpase_time_rewrite = time.time() - before_rewrite

        chat_history_msgs=[]
        TRACE_LOGGER.trace(f'**Rewrite: {origin_query} => {query_input}, elpase_time:{elpase_time_rewrite:.3f}**')
        logger.info(f'Rewrite: {origin_query} => {query_input}')
        #add history parameter

        chat_history= get_chat_history(chat_coversions[-2:])
        chat_history_msgs= history_to_messages(chat_coversions[-2:])
    else:
        chat_history = ''
        chat_history_msgs= []


    
    doc_retriever = CustomDocRetriever.from_endpoints(embedding_model_endpoint=embedding_model_endpoint,
                                    aos_endpoint= aos_endpoint,
                                    aos_index=aos_index)
    
    TRACE_LOGGER.trace(f'**Using LLM model : {llm_model_name}**')
    cache_answer = None
    if use_qa:
        before_prefetch = time.time()
        TRACE_LOGGER.trace(f'**Prefetching cache...**')
        cache_repsonses = doc_retriever.knn_quick_prefetch(query_input=query_input, prefetch_threshold=KNN_QUICK_PEFETCH_THRESHOLD)
        elpase_time_cache = time.time() - before_prefetch
        TRACE_LOGGER.trace(f'**Running time of prefetching cache: {elpase_time_cache:.3f}s**')
        if cache_repsonses:
            last_cache = cache_repsonses[-1]['doc']
            cache_answer = last_cache.split('\nAnswer:')[1]
            TRACE_LOGGER.trace(f"**Found caches:**")
            for sn,item in enumerate(cache_repsonses[::-1]):
                TRACE_LOGGER.trace(f"**[{sn+1}] [{item['doc_title']}] [{item['doc_type']}] [{item['doc_classify']}] [{item['score']:.3f}] author:[{item['doc_author']}]**")
                TRACE_LOGGER.trace(f"{item['doc']}")
        else:
            TRACE_LOGGER.trace(f"**No cache found**")

    detection = {'func': 'QA'}
    intention = detection['func']
    global INTENTION_LIST
    other_intentions = INTENTION_LIST.split(',')

    ##å¦‚æœä½¿ç”¨QAï¼Œä¸”æ²¡æœ‰cache answerå†éœ€è¦è¿›ä¸€æ­¥æ„å›¾åˆ¤æ–­
    if use_qa and not cache_answer and len(other_intentions) > 0 and len(other_intentions[0]) > 1:
        before_detect = time.time()
        TRACE_LOGGER.trace(f'**Detecting intention...**')
        detection = detect_intention(query_input,example_index, fewshot_cnt=5)
        intention = detection['func']
        elpase_time_detect = time.time() - before_detect
        logger.info(f'detection: {detection}')
        logger.info(f'running time of detecting : {elpase_time_detect:.3f}s')
        TRACE_LOGGER.trace(f'**Running time of detecting: {elpase_time_detect:.3f}s**')
        TRACE_LOGGER.trace(f'**Detected intention: {intention}**')
    
    if not use_qa:
        intention = 'chat'

    if cache_answer:
        TRACE_LOGGER.trace('**Use Cache answer:**')
        reply_stratgy = ReplyStratgy.OTHER
        answer = cache_answer
        if use_stream:
            TRACE_LOGGER.postMessage(cache_answer)
        recall_knowledge,opensearch_knn_respose,opensearch_query_response = [],[],[]
    elif intention in ['comfort', 'transfer']:
        reply_stratgy = ReplyStratgy.OTHER
        TRACE_LOGGER.trace('**Answer:**')
        answer = ''
        if intention == 'comfort':
            answer = "ä¸å¥½æ„æ€æ²¡èƒ½å¸®åˆ°æ‚¨ï¼Œæ˜¯å¦å¸®ä½ è½¬äººå·¥å®¢æœï¼Ÿ"
        elif intention == 'transfer':
            answer = 'ç«‹å³ä¸ºæ‚¨è½¬äººå·¥å®¢æœï¼Œè¯·ç¨å'
        
        if use_stream:
            TRACE_LOGGER.postMessage(answer)

        recall_knowledge,opensearch_knn_respose,opensearch_query_response = [],[],[]
    elif intention in ['chat', 'assist']:##å¦‚æœä¸ä½¿ç”¨QA
        TRACE_LOGGER.trace(f'**Using Non-RAG {intention}...**')
        TRACE_LOGGER.trace('**Answer:**')
        reply_stratgy = ReplyStratgy.LLM_ONLY
        prompt_template = None
        answer = ''

        # for message api
        sys_msg = {"role": "system", "content": SYSTEM_ROLE_PROMPT } if SYSTEM_ROLE_PROMPT else None
        msg_list = [sys_msg, *chat_history_msgs] if sys_msg else [*chat_history_msgs]
        msg = format_to_message(query=origin_query, image_base64_list=images_base64)
        msg_list.append(msg)

        # for prompt api
        prompt_template = create_chat_prompt_templete(llm_model_name=llm_model_name)
        prompt = prompt_template.format(question=origin_query,role_bot=B_Role,chat_history=chat_history)

        ai_reply = invoke_model(llm=llm, prompt=prompt, messages=msg_list, callbacks=[stream_callback])

        final_prompt = json.dumps(msg_list,ensure_ascii=False)
        answer = ai_reply.content
        
        recall_knowledge,opensearch_knn_respose,opensearch_query_response = [],[],[]

    elif intention == 'QA': ##å¦‚æœä½¿ç”¨QA
        # 2. aos retriever
        TRACE_LOGGER.trace('**Using RAG Chat...**')
        

        # doc_retriever = CustomDocRetriever.from_endpoints(embedding_model_endpoint=embedding_model_endpoint,
        #                             aos_endpoint= aos_endpoint,
        #                             aos_index=aos_index)
        # 3. check is it keyword search
        # exactly_match_result = aos_search(aos_endpoint, aos_index, "doc", query_input, exactly_match=True)
        ## ç²¾å‡†åŒ¹é…å¯¹paragraphç±»å‹æ–‡æ¡£ä¸å¤ªé€‚ç”¨ï¼Œå…ˆå±è”½æ‰ 
        exactly_match_result = None

        start = time.time()
        ## åŠ ä¸Šä¸€è½®çš„é—®é¢˜æ‹¼æ¥æ¥å¬å›å†…å®¹
        # query_with_history= get_question_history(chat_coversions[-2:])+query_input
        recall_knowledge = None
        opensearch_knn_respose = []
        opensearch_query_response = []
        if KNOWLEDGE_BASE_ID:
            TRACE_LOGGER.trace('**Retrieving knowledge from bedrock knowledgebase...**')
            recall_knowledge = doc_retriever.get_relevant_documents_from_bedrock(
                    knowledge_base_id=KNOWLEDGE_BASE_ID, 
                    query_input=query_input, 
                    top_k=TOP_K, 
                    rerank_endpoint=CROSS_MODEL_ENDPOINT, 
                    rerank_threshold=RERANK_THRESHOLD
                )
        else:
            TRACE_LOGGER.trace('**Retrieving knowledge from OpenSearch...**')
            recall_knowledge, opensearch_knn_respose, opensearch_query_response = doc_retriever.get_relevant_documents_custom(
                query_input=query_input, 
                channel_return_cnt=CHANNEL_RET_CNT, 
                top_k=TOP_K, 
                knn_threshold=KNN_QQ_THRESHOLD_HARD_REFUSE, 
                bm25_threshold=BM25_QD_THRESHOLD_HARD_REFUSE, 
                web_search_threshold=WEBSEARCH_THRESHOLD, 
                use_search=use_search,
                rerank_endpoint=CROSS_MODEL_ENDPOINT,
                rerank_threshold=RERANK_THRESHOLD
            ) 

        elpase_time = time.time() - start
        logger.info(f'running time of opensearch_query : {elpase_time:.3f}s seconds')
        
        reply_stratgy = get_reply_stratgy(recall_knowledge,refuse_strategy)
        if reply_stratgy == ReplyStratgy.LLM_ONLY: 
            recall_knowledge = []
            
        TRACE_LOGGER.trace(f'**Running time of retrieving knowledge : {elpase_time:.3f}s**')
        TRACE_LOGGER.trace(f'**Retrieved {len(recall_knowledge)} knowledge:**')
        TRACE_LOGGER.add_ref(f'\n\n**Refer to {len(recall_knowledge)} knowledge:**')

        ##æ·»åŠ å¬å›æ–‡æ¡£åˆ°refdocå’Œtracelog, æŒ‰scoreå€’åºå±•ç¤º
        for sn,item in enumerate(recall_knowledge[::-1]):
            TRACE_LOGGER.trace(f"**[{sn+1}] [{item['doc_title']}] [{item['doc_classify']}] [{item['score']:.3f}] [{item['rank_score']:.3f}] author:[ {item['doc_author']} ]**")
            TRACE_LOGGER.trace(f"{item['doc']}")
            TRACE_LOGGER.add_ref(f"**[{sn+1}] [{item['doc_title']}] [{item['doc_classify']}] [{item['score']:.3f}] [{item['rank_score']:.3f}] author:[ {item['doc_author']} ]**")
            #doc å¤ªé•¿ä¹‹åè¿›è¡Œæˆªæ–­
            TRACE_LOGGER.add_ref(f"{item['doc'][:500]}{'...' if len(item['doc'])>500 else ''}") 
        TRACE_LOGGER.trace('**Answer:**')

        if exactly_match_result and recall_knowledge:
            answer = exactly_match_result[0]["doc"]
            hide_ref= True ## éšè—ref doc
            if use_stream:
                TRACE_LOGGER.postMessage(answer)
                
        elif reply_stratgy == ReplyStratgy.RETURN_OPTIONS:
            some_reference, multi_choice_field = format_knowledges(recall_knowledge[::2])
            answer = f"æˆ‘ä¸å¤ªç¡®å®šï¼Œè¿™æœ‰ä¸¤æ¡å¯èƒ½ç›¸å…³çš„ä¿¡æ¯ï¼Œä¾›å‚è€ƒï¼š\n=====\n{some_reference}\n====="
            hide_ref= True ## éšè—ref doc
            if use_stream:
                TRACE_LOGGER.postMessage(answer)
                
        elif reply_stratgy == ReplyStratgy.SAY_DONT_KNOW:
            answer = refuse_answer
            hide_ref= True ## éšè—ref doc
            if use_stream:
                TRACE_LOGGER.postMessage(answer)
                
        elif reply_stratgy == ReplyStratgy.LLM_ONLY: ##èµ°LLMé»˜è®¤çŸ¥è¯†
            # prompt_template = create_chat_prompt_templete()
            # hide_ref= True ## éšè—ref doc
            # llmchain = LLMChain(llm=llm,verbose=verbose,prompt =prompt_template )
            # ##æœ€ç»ˆçš„answer
            # answer = llmchain.run({'question':query_input,'chat_history':chat_history,'role_bot':B_Role})
            # ##æœ€ç»ˆçš„promptæ—¥å¿—
            
            # for message api
            sys_msg = {"role": "system", "content": SYSTEM_ROLE_PROMPT } if SYSTEM_ROLE_PROMPT else None
            msg_list = [sys_msg, *chat_history_msgs] if sys_msg else [*chat_history_msgs]
            msg = format_to_message(query=origin_query, image_base64_list=images_base64)
            msg_list.append(msg)

            # for prompt api
            prompt_template = create_chat_prompt_templete(llm_model_name=llm_model_name)
            prompt = prompt_template.format(question=origin_query,role_bot=B_Role,chat_history=chat_history)

            ai_reply = invoke_model(llm=llm, prompt=prompt, messages=msg_list, callbacks=[stream_callback])

            final_prompt = json.dumps(msg_list,ensure_ascii=False)
            answer = ai_reply.content
            
        else:      
            prompt_template = create_qa_prompt_templete(template)
            # llmchain = LLMChain(llm=llm,verbose=verbose,prompt =prompt_template )

            # context = "\n".join([doc['doc'] for doc in recall_knowledge])
            context, multi_choice_field = format_knowledges(recall_knowledge)
            ask_user_prompts = [ f"- If you are not sure about which {field} user ask for, please ask user to clarify it before giving any answer, don't say anything else." for field in multi_choice_field ]
            ask_user_prompts_str = "\n".join(ask_user_prompts)
            if len(ask_user_prompts_str) > 0:
                ask_user_prompts_str = f"\n{ask_user_prompts_str}\n"

            try:
                chat_history = '' ##QA åœºæ™¯ä¸‹å…ˆä¸ä½¿ç”¨history
                prompt = prompt_template.format(question=query_input,role_bot=B_Role,context=context,chat_history=chat_history,ask_user_prompt=ask_user_prompts_str)
                
                sys_msg = {"role": "system", "content": SYSTEM_ROLE_PROMPT } if SYSTEM_ROLE_PROMPT else None
                msg_list = [sys_msg, *chat_history_msgs] if sys_msg else [*chat_history_msgs]
                msg = format_to_message(query=prompt, image_base64_list=images_base64)
                msg_list.append(msg)

                ai_reply = invoke_model(llm=llm, prompt=prompt, messages=msg_list, callbacks=[stream_callback])
                final_prompt = json.dumps(msg_list,ensure_ascii=False)
                answer = ai_reply.content
            except Exception as e:
                answer = str(e)
    else:
        #call agent for other intentions
        TRACE_LOGGER.trace('**Using Agent...**')
        reply_stratgy = ReplyStratgy.AGENT

        answer,ref_doc = chat_agent(query_input, detection)
        recall_knowledge,opensearch_knn_respose,opensearch_query_response = [ref_doc],[],[]
        TRACE_LOGGER.add_ref(f'\n\n**Refer to {len(recall_knowledge)} knowledge:**')
        TRACE_LOGGER.add_ref(f"**[1]** {ref_doc}")
        TRACE_LOGGER.trace(f'**Function call result:**\n\n{ref_doc}')
        TRACE_LOGGER.trace('**Answer:**')
        if use_stream:
            TRACE_LOGGER.postMessage(answer)

    answer = enforce_stop_tokens(answer, STOP)
    pattern = r'^æ ¹æ®[^ï¼Œ,]*[,|ï¼Œ]'
    answer = re.sub(pattern, "", answer.strip())
    ref_text = ''
    # if not use_stream and recall_knowledge and hide_ref == False:
        # ref_text = format_reference(recall_knowledge)
    ref_text = TRACE_LOGGER.dump_refs(use_stream)

    json_obj = {
        "query": query_input,
        "origin_query" : origin_query,
        "intention" : intention,
        "opensearch_doc":  opensearch_query_response,
        "opensearch_knn_doc":  opensearch_knn_respose,
        "kendra_doc": [],
        "knowledges" : recall_knowledge,
        "LLM_input": final_prompt,
        "LLM_model_name": llm_model_name,
        "reply_stratgy" : reply_stratgy.name,
        "use_search":use_search,
        "aos_index":aos_index,
    }
    json_obj['user_id'] = user_id
    json_obj['session_id'] = session_id
    json_obj['msgid'] = msgid
    json_obj['chatbot_answer'] = answer
    json_obj['ref_docs'] = ref_text
    json_obj['conversations'] = chat_coversions[-1:]
    json_obj['timestamp'] = int(time.time())
    json_obj['log_type'] = "all"
    json_obj_str = json.dumps(json_obj, ensure_ascii=False)
    logger.info(json_obj_str)

    start = time.time()
    if session_id != 'OnlyForDEBUG':
        update_session(session_id=session_id,user_id=user_id, question=origin_query, answer=answer, intention=intention, msgid=msgid)
    elpase_time = time.time() - start
    elpase_time1 = time.time() - start1
    logger.info(f'running time of update_session : {elpase_time}s seconds')
    logger.info(f'running time of all  : {elpase_time1}s seconds')
    return answer,ref_text,use_stream,query_input,opensearch_query_response,opensearch_knn_respose,recall_knowledge

def get_s3_image_base64(bucket_name, key):
    # Create an S3 client
    s3 = boto3.client('s3')
    # Get the object from S3
    try:
        response = s3.get_object(Bucket=bucket_name, Key=key)
        image_data = response['Body'].read()
        # Encode the image data as base64
        base64_image = base64.b64encode(image_data).decode('utf-8')
        return base64_image
    except Exception as e:
        print(f"Error getting object from S3: {e}")
        return None

def generate_s3_image_url(bucket_name, key, expiration=3600):
    s3_client = boto3.client('s3')
    url = s3_client.generate_presigned_url(
        'get_object',
         Params={'Bucket': bucket_name, 'Key': key},
         ExpiresIn=expiration
    )
    return url



@handle_error
def lambda_handler(event, context):
    # "model": æ¨¡å‹çš„åç§°
    # "chat_name": å¯¹è¯æ ‡è¯†ï¼Œåç«¯ç”¨æ¥å­˜å‚¨æŸ¥æ‰¾å®ç°å¤šè½®å¯¹è¯ session
    # "prompt": ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    # "max_tokens": 2048
    # "temperature": 0.9
    logger.info(f"event:{event}")
    method = event.get('method')
    resource = event.get('resource')
    global CHANNEL_RET_CNT
    CHANNEL_RET_CNT = event.get('channel_cnt', 10)
    logger.info(f'channel_cnt:{CHANNEL_RET_CNT}')

    ###å…¶ä»–ç®¡ç†æ“ä½œ start
    ###å…¶ä»–ç®¡ç†æ“ä½œ start
    if resource:
        ret_json = management_api(method,resource,event)
        return ret_json

    ####å…¶ä»–ç®¡ç†æ“ä½œ end

    # input_json = json.loads(event['body'])
    ws_endpoint = event.get('ws_endpoint')
    if ws_endpoint:
        wsclient = boto3.client('apigatewaymanagementapi', endpoint_url=ws_endpoint)
    else:
        wsclient = None
    global openai_api_key
    openai_api_key = event.get('OPENAI_API_KEY') 
    hide_ref = event.get('hide_ref',False)
    feature_config = event.get('feature_config','')
    retrieve_only = event.get('retrieve_only',False)
    session_id = event['chat_name']
    wsconnection_id = event.get('wsconnection_id',session_id)
    question = event['prompt']
    model_name = event['model'] if event.get('model') else event.get('model_name','')
    embedding_endpoint = event.get('embedding_model',os.environ.get("embedding_endpoint")) 
    use_qa = event.get('use_qa',False)
    multi_rounds = event.get('multi_rounds',False)
    use_stream = event.get('use_stream',False)
    user_id = event.get('user_id','')
    use_trace = event.get('use_trace',True)
    template_id = event.get('template_id')
    msgid = event.get('msgid')
    max_tokens = event.get('max_tokens',2048)
    temperature =  event.get('temperature',0.1)
    aos_index = os.environ.get("aos_index", "")
    company = event.get("company",'default')
    example_index = "chatbot-example-index"
    aos_index = f'chatbot-index-{company}'
    example_index = f'chatbot-example-index-{company}'
    refuse_strategy = event.get('refuse_strategy','')
    refuse_answer = event.get('refuse_answer','å¯¹ä¸èµ·ï¼Œæˆ‘ä¸å¤ªæ¸…æ¥šè¿™ä¸ªé—®é¢˜ï¼Œè¯·é—®é—®äººå·¥å§')
        
    
    imgurls = event.get('imgurl')
    image_path = ''
    images_base64 = []
    if imgurls:
        logger.info(f"imgurls:{imgurls}")
        for imgurl in imgurls:
            bucket,imgobj = imgurl.split('/',1)
            # image_path = generate_s3_image_url(bucket,imgobj)
            image_base64 = get_s3_image_base64(bucket,imgobj)
            images_base64.append(image_base64)

    ## ç”¨äºtrulengthæ¥å£ï¼Œåªè¿”å›recall çŸ¥è¯†
    if retrieve_only:
        doc_retriever = CustomDocRetriever.from_endpoints(embedding_model_endpoint=embedding_endpoint,
                                    aos_endpoint= os.environ.get("aos_endpoint", ""),
                                    aos_index=aos_index)
        recall_knowledge,opensearch_knn_respose,opensearch_query_response = doc_retriever.get_relevant_documents_custom(
                query_input=question, 
                channel_return_cnt=CHANNEL_RET_CNT, 
                top_k=TOP_K, 
                knn_threshold=KNN_QQ_THRESHOLD_HARD_REFUSE, 
                bm25_threshold=BM25_QD_THRESHOLD_HARD_REFUSE, 
                web_search_threshold=WEBSEARCH_THRESHOLD, 
                use_search=use_search,
                rerank_endpoint=CROSS_MODEL_ENDPOINT,
                rerank_threshold=RERANK_THRESHOLD
            ) 
        extra_info = {"query_input": question, "opensearch_query_response" : opensearch_query_response, "opensearch_knn_respose": opensearch_knn_respose,"recall_knowledge":recall_knowledge }
        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json'},
            'body': [{"id": str(uuid.uuid4()), "extra_info" : extra_info,} ]
        }
        
    ##è·å–å‰ç«¯ç»™çš„ç³»ç»Ÿè®¾å®šï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨lambdaé‡Œçš„é»˜è®¤å€¼
    global B_Role,SYSTEM_ROLE_PROMPT
    B_Role = event.get('system_role',B_Role)
    SYSTEM_ROLE_PROMPT = event.get('system_role_prompt',SYSTEM_ROLE_PROMPT)
    
    logger.info(f'system_role:{B_Role},system_role_prompt:{SYSTEM_ROLE_PROMPT}')

    llm_endpoint = os.environ.get('llm_model_endpoint')


    # è·å–å½“å‰æ—¶é—´æˆ³
    request_timestamp = time.time()  # æˆ–è€…ä½¿ç”¨ time.time_ns() è·å–çº³ç§’çº§åˆ«çš„æ—¶é—´æˆ³
    logger.info(f'request_timestamp :{request_timestamp}')
    logger.info(f"event:{event}")
    # logger.info(f"context:{context}")

    # æ¥æ”¶è§¦å‘AWS Lambdaå‡½æ•°çš„äº‹ä»¶
    logger.info('The main brain has been activated, awsğŸš€!')

    # 1. è·å–ç¯å¢ƒå˜é‡

    # embedding_endpoint = os.environ.get("embedding_endpoint", "")
    aos_endpoint = os.environ.get("aos_endpoint", "")
    aos_knn_field = os.environ.get("aos_knn_field", "")
    aos_result_num = int(os.environ.get("aos_results", 4))

    Kendra_index_id = os.environ.get("Kendra_index_id", "")
    Kendra_result_num = int(os.environ.get("Kendra_result_num", 0))
    # Opensearch_result_num = int(os.environ.get("Opensearch_result_num", ""))
    prompt_template = ''

    ##å¦‚æœæŒ‡å®šäº†prompt æ¨¡æ¿
    if template_id and template_id != 'default':
        prompt_template = get_template(template_id,company)
        prompt_template = '' if prompt_template is None else prompt_template['template']['S']
    
    use_search = False if feature_config == 'search_disabled' else True
    logger.info(f'use_search : {use_search}')
    logger.info(f'user_id : {user_id}')
    logger.info(f'prompt_template_id : {template_id}')
    logger.info(f'prompt_template : {prompt_template}')
    logger.info(f'model_name : {model_name}')
    logger.info(f'llm_endpoint : {llm_endpoint}')
    logger.info(f'embedding_endpoint : {embedding_endpoint}')
    logger.info(f'aos_endpoint : {aos_endpoint}')
    logger.info(f'aos_index : {aos_index}')
    logger.info(f'aos_knn_field : {aos_knn_field}')
    logger.info(f'aos_result_num : {aos_result_num}')
    logger.info(f'Kendra_index_id : {Kendra_index_id}')
    logger.info(f'Kendra_result_num : {Kendra_result_num}')
    logger.info(f'use multiple rounds: {multi_rounds}')
    logger.info(f'intention list: {INTENTION_LIST}')
    logger.info(f'refuse_strategy: {refuse_strategy}')
    logger.info(f'refuse_answer: {refuse_answer}')
    
    ##if aos and bedrock kb are null then set use_qa = false
    if not aos_endpoint and not KNOWLEDGE_BASE_ID:
        use_qa = False
        logger.info(f'force set use_qa: {use_qa}')
        
    global TRACE_LOGGER
    TRACE_LOGGER = TraceLogger(wsclient=wsclient,msgid=msgid,connectionId=wsconnection_id,stream=use_stream,use_trace=use_trace,hide_ref=hide_ref)
    main_entry_start = time.time()  # æˆ–è€…ä½¿ç”¨ time.time_ns() è·å–çº³ç§’çº§åˆ«çš„æ—¶é—´æˆ³
    answer,ref_text,use_stream,query_input,opensearch_query_response,opensearch_knn_respose,recall_knowledge = main_entry_new(user_id,wsconnection_id,session_id, question, embedding_endpoint, llm_endpoint, model_name, aos_endpoint, aos_index, aos_knn_field, aos_result_num,
                       Kendra_index_id, Kendra_result_num,use_qa,wsclient,msgid,max_tokens,temperature,prompt_template,images_base64,multi_rounds,hide_ref,use_stream,example_index,use_search,refuse_strategy,refuse_answer)
    main_entry_elpase = time.time() - main_entry_start  # æˆ–è€…ä½¿ç”¨ time.time_ns() è·å–çº³ç§’çº§åˆ«çš„æ—¶é—´æˆ³
    logger.info(f'running time of main_entry : {main_entry_elpase}s seconds')
    if use_stream: ##åªæœ‰å½“streamè¾“å‡ºæ—¶ï¼ŒæŠŠè¿™æ¡traceæ”¾åˆ°æœ€åä¸€ä¸ªchunk
        TRACE_LOGGER.trace(f'\n\n**Total running time : {main_entry_elpase:.3f}s**')
    if TRACE_LOGGER.use_trace:
        tracelogs_str = TRACE_LOGGER.dump_logs_to_string()
        ## è¿”å›éstreamçš„ç»“æœï¼ŒæŠŠè¿™æ¡traceæ”¾åˆ°æœ«å°¾
        answer =f'{tracelogs_str}\n\n{answer}\n\n**Total running time : {main_entry_elpase:.3f}s**'
    else:
        answer = answer if hide_ref else f'{answer}{ref_text}'
    # 2. return rusult

    # å¤„ç†

    # Response:
    # "id": "è®¾ç½®ä¸€ä¸ªuuid"
    # "created": "1681891998"
    # "model": "æ¨¡å‹åç§°"
    # "choices": [{"text": "æ¨¡å‹å›ç­”çš„å†…å®¹"}]
    # "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}}]
    extra_info = {}
    if session_id == 'OnlyForDEBUG':
        extra_info = {"query_input": query_input, "opensearch_query_response" : opensearch_query_response, "opensearch_knn_respose": opensearch_knn_respose,"recall_knowledge":recall_knowledge }
    return {
        'statusCode': 200,
        'headers': {'Content-Type': 'application/json'},
        'body': [{"id": str(uuid.uuid4()),
                  "use_stream":use_stream,
                  "query":query_input,
                             "created": request_timestamp,
                             "useTime": time.time() - request_timestamp,
                             "model": "main_brain",
                             "choices": [{"text": answer}],
                            #  [{"text": "{}[{}]".format(answer, model_name)}],
                             "extra_info" : extra_info,
                             "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}},
                            # {"id": uuid.uuid4(),
                            #  "created": request_timestamp,
                            #  "useTime": int(time.time()) - request_timestamp,
                            #  "model": "æ¨¡å‹åç§°",
                            #  "choices":
                            #  [{"text": "2 æ¨¡å‹å›ç­”çš„å†…å®¹"}],
                            #  "usage": {"prompt_tokens": 58, "completion_tokens": 15, "total_tokens": 73}}
                            ]
    }